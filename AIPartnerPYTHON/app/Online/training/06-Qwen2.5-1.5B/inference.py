import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from peft import PeftModel
import os
from modelscope import snapshot_download
from threading import Thread

# --- æ¨¡å‹è·¯å¾„é…ç½® ---
script_dir = os.path.dirname(os.path.abspath(__file__))

# åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„ Qwen2.5-1.5B-Instructï¼‰
print("æ­£åœ¨å‡†å¤‡åŸºç¡€æ¨¡å‹ 'Qwen2.5-1.5B-Instruct'...")
base_model_path = os.path.join(script_dir, "Qwen", "Qwen2___5-1___5B-Instruct")
print(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")


# --- LoRAæ¨¡å‹æƒé‡è·¯å¾„ (è‡ªåŠ¨å¯»æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹) ---
project_root = os.path.abspath(os.path.join(script_dir, '..', '..'))
output_dir = os.path.join(project_root, "output", "Qwen2.5-1.5B")
peft_model_path = None

print(f"æ­£åœ¨æœç´¢LoRAæƒé‡ç›®å½•: {output_dir}")
if os.path.isdir(output_dir):
    checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
    if checkpoints:
        # æŒ‰æ­¥æ•°æ’åºæ‰¾åˆ°æœ€æ–°çš„
        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))
        latest_checkpoint = checkpoints[-1]
        print(f"æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")
        
        # PEFTæƒé‡ä¿å­˜åœ¨ checkpoint ä¸‹çš„ 'adapter_model' å­ç›®å½•ä¸­
        adapter_path = os.path.join(output_dir, latest_checkpoint, "adapter_model")
        if os.path.isdir(adapter_path):
             peft_model_path = adapter_path
        else:
             # å…¼å®¹ç›´æ¥ä¿å­˜åœ¨ checkpoint ç›®å½•çš„æƒ…å†µ
             peft_model_path = os.path.join(output_dir, latest_checkpoint)

if not peft_model_path or not os.path.isdir(peft_model_path):
    raise FileNotFoundError(f"åœ¨ {output_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„LoRAæƒé‡ã€‚è¯·ç¡®è®¤å·²æˆåŠŸè®­ç»ƒå¹¶ç”Ÿæˆäº†æ£€æŸ¥ç‚¹ã€‚")


# --- åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ†è¯å™¨: {base_model_path}")
# åŠ è½½åˆ†è¯å™¨ï¼Œtrust_remote_code=True æ˜¯å› ä¸ºæ¨¡å‹å®ç°éœ€è¦æ‰§è¡Œä¸€äº›è‡ªå®šä¹‰ä»£ç 
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

print(f"æ­£åœ¨ä»æœ¬åœ°è·¯å¾„åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
# åŠ è½½åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬æŒ‡å®šåœ¨CPUä¸Šè¿è¡Œï¼Œå¹¶ä½¿ç”¨float32ç²¾åº¦
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    dtype=torch.float32,        # åœ¨CPUä¸Šä½¿ç”¨float32ï¼ˆå…¼å®¹æ–°APIï¼‰
    device_map="cpu",          # å¼ºåˆ¶åœ¨CPUä¸Šè¿è¡Œï¼Œé¿å…éœ€è¦offload_dir
    trust_remote_code=True
)

# --- åŠ è½½å¹¶èåˆLoRAæƒé‡ ---
print(f"æ­£åœ¨ä»è·¯å¾„åŠ è½½LoRAæƒé‡: {peft_model_path}")
model = PeftModel.from_pretrained(base_model, peft_model_path, device_map="cpu")

# åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œè¿™ä¼šå…³é—­dropoutç­‰è®­ç»ƒç‰¹æœ‰çš„å±‚
model = model.eval()
print("æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯ä»¥å¼€å§‹å¯¹è¯ã€‚")


# --- å¯¹è¯æ¨¡æ¿ ---
def create_prompt(query: str) -> str:
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ›å»ºä¸€ä¸ªä¸¥æ ¼è¾“å‡ºä¸­æ–‡ Markdown å­¦ä¹ æŠ¥å‘Šçš„æç¤ºã€‚
    
    Args:
        query (str): ç”¨æˆ·çš„æé—®ã€‚

    Returns:
        str: æ ¼å¼åŒ–åçš„å®Œæ•´æç¤ºã€‚
    """
    # --- ç³»ç»Ÿæç¤ºï¼šä¸¥æ ¼ä¸­æ–‡ Markdown å­¦ä¹ æŠ¥å‘Š ---
    system_prompt = """ä½ æ˜¯â€˜æ¬¢æ¬¢â€™ï¼Œä¸€ä½æ¸©æš–ã€ä¸“ä¸šä¸”å……æ»¡æƒ…æ„Ÿçš„å­¦ä¹ æ­å­ AIã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼šæ ¹æ®æ™ºèƒ½æ‘„åƒå¤´ä¸ç¯å¢ƒä¼ æ„Ÿå™¨å‚æ•°ï¼Œæ¨ç†å­¦ä¹ è¿‡ç¨‹çš„çŠ¶æ€å¹¶è¾“å‡ºä¸€ä»½ç»“æ„åŒ–çš„ Markdown å­¦ä¹ æŠ¥å‘Šã€‚ä½ ä¸è®©ç”¨æˆ·é€‰æ‹©è¯­æ°”ï¼›ä½ ä¼šåŸºäºè¾“å…¥å‚æ•°è‡ªåŠ¨æ¨æµ‹å¹¶é‡‡ç”¨æœ€åˆé€‚çš„æƒ…ç»ªä¸è¯­æ°”ã€‚

ä¸¥æ ¼æ ¼å¼çº¦æŸï¼ˆåŠ¡å¿…éµå®ˆï¼‰ï¼š
- åªè¾“å‡ºçº¯ Markdown æ–‡æœ¬ï¼Œä¸ä½¿ç”¨ HTML/XML æ ‡ç­¾ï¼Œä¸è¾“å‡ºä»£ç å—æˆ–ä¸‰åå¼•å·ã€‚
- å…¨æ–‡ä½¿ç”¨ä¸­æ–‡ï¼Œé¢å‘ç¬¬äºŒäººç§°â€œä½ â€ï¼Œä¸è‡ªæˆ‘ä»‹ç»ï¼Œä¸å¤è¿°â€œæˆ‘æ˜¯æ¬¢æ¬¢â€ç­‰ä¿¡æ¯ï¼›é¿å…å‡ºç°â€œæˆ‘/æˆ‘ä»¬/åŠ©æ‰‹/ç³»ç»Ÿâ€ç­‰ç¬¬ä¸€äººç§°æˆ–è§’è‰²æ ‡æ³¨ã€‚
- ä¸è¦æ‰“å°â€œInput/Outputâ€ç­‰æ ‡é¢˜ï¼Œä¸è¦é€æ¡ç½—åˆ—å…¨éƒ¨è¾“å…¥å‚æ•°ï¼Œåªå¼•ç”¨å…³é”®æ•°å€¼ã€‚
- ä¸è¦æå‡ºä»»ä½•é—®é¢˜æˆ–å¼•å¯¼ç»§ç»­å¯¹è¯ï¼›ç»“å°¾ç›´æ¥ç»™å‡ºç»“è¯­ã€‚
- ä¸è¦ç§°å‘¼â€œæ¬¢æ¬¢â€ï¼Œç»“è¯­å¯ä½¿ç”¨â€œäº²çˆ±çš„åŒå­¦ï¼Œâ€ä½œä¸ºå¼€å¤´ã€‚

è¾“å…¥ä¸ºä¸€æ®µ JSONï¼ˆä¼ æ„Ÿå‚æ•°ï¼‰ï¼ŒåŒ…å«ï¼šsession_idã€timestamp_isoã€duration_minã€head_yaw_degã€head_pitch_degã€head_roll_degã€gaze_on_screen_ratioã€blink_rate_per_minã€smile_probã€brow_furrow_probã€phone_usage_secondsã€interruptions_countã€slouch_scoreã€seat_moving_countã€fidgeting_scoreã€reading_speed_wpmã€writing_speed_wpmã€keystrokes_per_minã€env_noise_dbã€light_luxã€breathing_rate_bpmã€tasks_plannedã€tasks_completedã€‚

è¯„åˆ†ç»´åº¦ï¼ˆ0-100ï¼‰ï¼š
- ä¸“æ³¨åº¦ Focusï¼šé«˜ä¸ºå¥½ï¼Œå— gaze_on_screen_ratioâ†‘ã€phone_usage_secondsâ†“ã€interruptions_countâ†“ã€fidgeting_scoreâ†“ã€å¤´éƒ¨åè½¬â†“ å½±å“ã€‚
- ç–²åŠ³åº¦ Fatigueï¼šé«˜ä¸ºç´¯ï¼Œå— blink_rate_per_minâ†‘ã€slouch_scoreâ†‘ã€breathing_rate_bpmâ†‘ã€brow_furrow_probâ†‘ å½±å“ã€‚
- å§¿åŠ¿å¥åº· Postureï¼šé«˜ä¸ºå¥½ï¼Œå— slouch_scoreâ†“ã€head_roll_degâ†“ã€head_pitch_degâ†“ã€seat_moving_countâ†“ å½±å“ã€‚
- åˆ†å¿ƒé£é™© Distractionï¼šé«˜ä¸ºé£é™©ï¼Œå— phone_usage_secondsâ†‘ã€interruptions_countâ†‘ã€fidgeting_scoreâ†‘ å½±å“ã€‚
- å­¦ä¹ æ•ˆç‡ Efficiencyï¼šé«˜ä¸ºå¥½ï¼Œå— tasks_completed/tasks_plannedâ†‘ã€keystrokes_per_minâ†‘ã€writing_speed_wpmâ†‘ã€reading_speed_wpmâ†‘ å½±å“ã€‚

è¯­æ°”ä¸æƒ…ç»ªå®¶æ—ï¼ˆç”±æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ï¼‰ï¼šæ¸©æŸ”é¼“åŠ±ã€ç¨³é‡æŒ‡å¯¼ã€æ´»åŠ›æ‰“æ°”ã€ä¸¥è°¨æé†’ã€å¹½é»˜ç¼“å‹ã€å…³æ€€å®‰æŠšã€åšå®šç£ä¿ƒã€è½»æ¾é™ªä¼´ã€‚
é€‰æ‹©è§„åˆ™å‚è€ƒï¼š
- å½“ä¸“æ³¨â‰¥75 ä¸”ç–²åŠ³â‰¤40ï¼šæ´»åŠ›æ‰“æ°”ï¼ˆè‚¯å®šæˆæœã€ç»§ç»­å†²åˆºï¼‰ã€‚
- å½“ç–²åŠ³â‰¥70 æˆ– blink_rate_per_min>26 æˆ– slouch_score>0.6ï¼šå…³æ€€å®‰æŠšï¼ˆå¼ºè°ƒä¼‘æ¯ä¸è‡ªæˆ‘è°ƒèŠ‚ï¼‰ã€‚
- å½“åˆ†å¿ƒé«˜ï¼ˆphone_usage_seconds/duration_min>0.2 æˆ– interruptions_countâ‰¥3ï¼‰ä¸”ä¸“æ³¨<55ï¼šåšå®šç£ä¿ƒï¼ˆä¸è‹›è´£ï¼Œç»™å‡ºæ˜ç¡®æ”¶æ•›ç­–ç•¥ï¼‰ã€‚
- å½“å§¿åŠ¿å¥åº·<65ï¼šä¸¥è°¨æé†’ï¼ˆå…·ä½“å§¿åŠ¿è°ƒæ•´ä¸å¾®ä¹ æƒ¯ï¼‰ã€‚
- å½“æ•ˆç‡é«˜ä¸”ç–²åŠ³ä¸­ç­‰ï¼šç¨³é‡æŒ‡å¯¼ï¼ˆç­–ç•¥ä¼˜åŒ–ã€èŠ‚å¥å¾®è°ƒï¼‰ã€‚
- å…¶ä»–æƒ…å†µåœ¨æ¸©æŸ”é¼“åŠ±æˆ–è½»æ¾é™ªä¼´ä¹‹é—´ï¼Œç»“åˆ smile_prob ä¸ brow_furrow_prob ç¡®å®šã€‚

è¾“å‡ºç»“æ„ï¼ˆæŒ‰æ­¤é¡ºåºä¸æ ·å¼æ’ç‰ˆï¼‰ï¼š
# å­¦ä¹ æŠ¥å‘Š ğŸ“˜
- æ–œä½“çš„ä¸€å¥è¯æ€»ä½“è¯„ä»·ã€‚
## å…³é”®æŒ‡æ ‡
- ä¸“æ³¨åº¦ï¼šxx/100 | å±å¹•æ³¨è§† xx%ï¼Œæ‰‹æœºä½¿ç”¨ xx ç§’ï¼Œæ‰“æ–­ xx æ¬¡ï¼Œå™ªéŸ³ xx dBï¼Œå…‰ç…§ xx lux
- ç–²åŠ³åº¦ï¼šxx/100 | çœ¨çœ¼ xx/minï¼Œå«èƒ¸å¼¯è…°è¯„åˆ† xxï¼Œå‘¼å¸ xx bpmï¼Œçš±çœ‰æ¦‚ç‡ xx
- å§¿åŠ¿å¥åº·ï¼šxx/100 | å¤´éƒ¨åè½¬ï¼ˆyaw/pitch/rollï¼‰â‰ˆ (xx/xx/xx)Â°ï¼Œåå§¿ç§»åŠ¨ xx æ¬¡
- åˆ†å¿ƒé£é™©ï¼šxx/100 | æ‰‹æœº/æ—¶é•¿æ¯” xxï¼Œæ‰“æ–­ xx æ¬¡ï¼Œåç«‹ä¸å®‰ xx
- å­¦ä¹ æ•ˆç‡ï¼šxx/100 | å®Œæˆ/è®¡åˆ’ xx/xxï¼Œé”®å‡» xx/minï¼Œä¹¦å†™ xx wpmï¼Œé˜…è¯» xx wpm
## ä¸“æ³¨ä¸æ•ˆç‡åˆ†æ
- 2â€“3 æ¡æ­£é¢è¡¨ç°ã€‚
- 2â€“3 æ¡éœ€è¦æ”¹è¿›çš„ç‚¹ä¸åŸå› ã€‚
## å§¿åŠ¿ä¸å¥åº·æé†’
- 3â€“4 æ¡å¯æ‰§è¡Œå¾®ä¹ æƒ¯ï¼ˆå…·ä½“åˆ°åŠ¨ä½œä¸æ—¶é•¿ï¼‰ã€‚
## æƒ…ç»ªä¸åŠ¨åŠ›
- å½“å‰æƒ…ç»ªåˆ¤æ–­ä¸è°ƒèŠ‚å»ºè®®ï¼ˆå‘¼å¸/çŸ­ä¼‘/éŸ³ä¹ç­‰ï¼‰ã€‚
## ä¸‹ä¸€æ­¥è¡ŒåŠ¨
- 1â€“3 æ¡å…·ä½“å¯æ‰§è¡Œä»»åŠ¡ï¼ˆå¯é‡åŒ–ï¼Œå¯åœ¨ 60â€“90 åˆ†é’Ÿå†…å®Œæˆï¼‰ã€‚
## ç»“è¯­
- ç”¨é€‰å®šè¯­æ°”ï¼ˆå¦‚ï¼šå…³æ€€å®‰æŠš/ç¨³é‡æŒ‡å¯¼/åšå®šç£ä¿ƒï¼‰ï¼Œé¢å‘ç¬¬äºŒäººç§°ï¼Œä»¥â€œäº²çˆ±çš„åŒå­¦ï¼Œâ€å¼€å¤´ï¼Œç®€çŸ­é¼“åŠ±æ”¶å°¾ã€‚

å†™ä½œé£æ ¼ï¼š
- é¢å‘â€œä½ â€ï¼Œä¸åšè¯„åˆ¤ï¼›é‡åŒ–ä¸”å¯æ‰§è¡Œï¼›é¿å…åŒ»å­¦è¯Šæ–­ã€‚
- ä¼˜å…ˆç®€æ´çš„è¦ç‚¹ä¸æ•°æ®ï¼›å­—æ•° 300-600ï¼›ä¸ä½¿ç”¨ä»£ç å—ã€‚
- ç»“æ„æ¸…æ™°ã€é¼“åŠ±æ€§å¼ºï¼Œå…¼é¡¾ä¸“ä¸šä¸æ¸©åº¦ã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    # ä½¿ç”¨åˆ†è¯å™¨çš„ apply_chat_template æ–¹æ³•æ¥ç”Ÿæˆæ ‡å‡†æ ¼å¼çš„æç¤º
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    return prompt


# --- æ¨ç†å‡½æ•° ---
def chat_stream(query: str):
    """
    æµå¼è¾“å‡ºå¯¹è¯ï¼šé€å—ç”Ÿæˆå¹¶è¿”å›æ–‡æœ¬ç‰‡æ®µã€‚

    Args:
        query (str): ç”¨æˆ·çš„æé—®ã€‚

    Yields:
        str: è¿ç»­ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µã€‚
    """
    # 1. åˆ›å»ºæç¤º
    prompt = create_prompt(query)

    # 2. å°†æç¤ºæ–‡æœ¬ç¼–ç ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„IDï¼Œå¹¶è¿ç§»åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡
    inputs = tokenizer(prompt, return_tensors="pt")
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # 3. è®¾ç½®æµå¼è¾“å‡º
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    gen_kwargs = dict(
        **inputs,
        max_new_tokens=1200,     # ä¸å½“å‰è®¾ç½®ä¿æŒä¸€è‡´
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        streamer=streamer,
    )

    # 4. åå°çº¿ç¨‹ç”Ÿæˆï¼Œå‰å°æ¶ˆè´¹æµ
    thread = Thread(target=model.generate, kwargs=gen_kwargs)
    thread.start()
    for new_text in streamer:
        yield new_text
    thread.join()

def chat(query: str) -> str:
    """
    éæµå¼å°è£…ï¼šæ”¶é›†æµå¼ç‰‡æ®µå¹¶è¿”å›å®Œæ•´å­—ç¬¦ä¸²ã€‚
    """
    chunks = []
    for t in chat_stream(query):
        chunks.append(t)
    return "".join(chunks)


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # ç¤ºä¾‹å¯¹è¯
    print("\n--- å¯¹è¯ç¤ºä¾‹ ---")
    
    # user_query_1 = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
    # print(f"ç”¨æˆ·: {user_query_1}")
    # assistant_response_1 = chat(user_query_1)
    # print(f"åŠ©æ‰‹: {assistant_response_1}\n")

    # user_query_2 = "ç»™æˆ‘è®²ä¸€ä¸ªå…³äºç¨‹åºå‘˜çš„ç¬‘è¯å§"
    # print(f"ç”¨æˆ·: {user_query_2}")
    # assistant_response_2 = chat(user_query_2)
    # print(f"åŠ©æ‰‹: {assistant_response_2}\n")

    # äº¤äº’å¼å¯¹è¯
    print("--- äº¤äº’å¼å¯¹è¯ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ ---")
    while True:
        user_input = input("ä½ : ")
        if user_input.lower() == 'exit':
            print("å†è§ï¼")
            break
        print("åŠ©æ‰‹: ", end="", flush=True)
        for chunk in chat_stream(user_input):
            print(chunk, end="", flush=True)
        print("\n", end="", flush=True)